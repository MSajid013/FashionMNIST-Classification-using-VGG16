{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5pKTPkEXRfx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3FIKsnGYsj4",
        "outputId": "3348384e-ab34-41f2-a6fc-ec1215213b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff057e678f0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using Device: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNHZL0eIZ9r7",
        "outputId": "525fb545-1b7d-4e19-c149-63e584400d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/fashion-mnist_train.csv')\n",
        "test = pd.read_csv('/content/fashion-mnist_test.csv')"
      ],
      "metadata": {
        "id": "W7WT2sYcaMLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train.iloc[:,1:].values\n",
        "y_train = train.iloc[:,0].values\n",
        "X_test = test.iloc[:,1:].values\n",
        "y_test = test.iloc[:,0].values"
      ],
      "metadata": {
        "id": "AGMWmApmaTov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "jQ72GRMWaWME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train/255.0\n",
        "X_test = X_test/255.0"
      ],
      "metadata": {
        "id": "hExubhb8bWPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Network"
      ],
      "metadata": {
        "id": "pDy08i1dxN9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, features, labels):\n",
        "    self.features = torch.tensor(features, dtype = torch.float32)\n",
        "    self.labels = torch.tensor(labels, dtype = torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.features[index], self.labels[index]"
      ],
      "metadata": {
        "id": "wi5x8iiKchNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "test_dataset = CustomDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "yzqEmCICdfim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset), len(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD1TDIbwdteU",
        "outputId": "96fa5dbe-d0b5-4cc7-8ec8-c681198ba4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define NN class\n",
        "class MyNN(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, num_hidden_layers, neurons_per_layer, dropout_rate):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    layers = []\n",
        "\n",
        "    for i in range(num_hidden_layers):\n",
        "\n",
        "      layers.append(nn.Linear(input_dim, neurons_per_layer))\n",
        "      layers.append(nn.BatchNorm1d(neurons_per_layer))\n",
        "      layers.append(nn.ReLU())\n",
        "      layers.append(nn.Dropout(dropout_rate))\n",
        "\n",
        "      input_dim = neurons_per_layer\n",
        "\n",
        "    layers.append(nn.Linear(neurons_per_layer, output_dim))\n",
        "\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, X):\n",
        "    return self.model(X)\n"
      ],
      "metadata": {
        "id": "pFQE_7JqfIqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "  # next hyperparameter values from the search space\n",
        "  num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 5)\n",
        "  neurons_per_layer = trial.suggest_int('neurons_per_layer', 8, 128, step=8)\n",
        "  epochs = trial.suggest_int('epochs', 10, 50, step=10)\n",
        "  learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
        "  weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
        "  dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
        "  batch_size = trial.suggest_categorical('batch_size', [16,32,64,128])\n",
        "  optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "  # model initialization\n",
        "  input_dim = 784\n",
        "  output_dim = 10\n",
        "\n",
        "  model = MyNN(input_dim, output_dim, num_hidden_layers, neurons_per_layer, dropout_rate)\n",
        "  model.to(device)\n",
        "\n",
        "  # optimizer selection\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  if optimizer_name == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
        "  elif optimizer_name == 'RMSprop':\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
        "  else:\n",
        "    optimizer = optim.SGD(model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "  # training loop\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    for batch_features, batch_labels in train_loader:\n",
        "\n",
        "      batch_features = batch_features.to(device)\n",
        "      batch_labels = batch_labels.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      outputs = model(batch_features)\n",
        "\n",
        "      # loss\n",
        "      loss = criterion(outputs, batch_labels)\n",
        "\n",
        "      # back pass\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      # update grads\n",
        "      optimizer.step()\n",
        "\n",
        "  # set model for evaluation\n",
        "  model.eval()\n",
        "\n",
        "  # evaluation code\n",
        "  total = 0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    for batch_features, batch_labels in test_loader:\n",
        "\n",
        "      batch_features = batch_features.to(device)\n",
        "      batch_labels = batch_labels.to(device)\n",
        "\n",
        "      outputs = model(batch_features)\n",
        "\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "      total = total + batch_labels.shape[0]\n",
        "\n",
        "      correct = correct + (predicted == batch_labels).sum().item()\n",
        "\n",
        "  return correct/total*100"
      ],
      "metadata": {
        "id": "NQWuh9jLmqkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "kLPmv7Gbpzrl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785fdbf7-7e92-41f8-c578-74122d5f8a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.18.1)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.7.0-py3-none-any.whl (413 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/413.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.9/413.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "study = optuna.create_study(direction = 'maximize')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vXDCmoVHonn",
        "outputId": "950ac4c0-922f-4db3-b3f5-37b5b05b8158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-30 14:05:57,565] A new study created in memory with name: no-name-eca450a3-03b0-4c91-8992-23c354dcd6c5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.optimize(objective, n_trials=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB_B5uSsH0GI",
        "outputId": "e39033f6-d343-49e6-f938-d0e1739ce8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-30 14:07:05,194] Trial 0 finished with value: 41.22 and parameters: {'num_hidden_layers': 4, 'neurons_per_layer': 128, 'epochs': 10, 'learning_rate': 2.116938332483784e-05, 'weight_decay': 1.0608295313646721e-05, 'dropout_rate': 0.5, 'batch_size': 32, 'optimizer': 'SGD'}. Best is trial 0 with value: 41.22.\n",
            "[I 2026-01-30 14:09:58,968] Trial 1 finished with value: 85.74000000000001 and parameters: {'num_hidden_layers': 3, 'neurons_per_layer': 24, 'epochs': 50, 'learning_rate': 0.0008574255473442948, 'weight_decay': 0.0008747936315415994, 'dropout_rate': 0.2, 'batch_size': 64, 'optimizer': 'SGD'}. Best is trial 1 with value: 85.74000000000001.\n",
            "[I 2026-01-30 14:16:08,923] Trial 2 finished with value: 83.19 and parameters: {'num_hidden_layers': 4, 'neurons_per_layer': 56, 'epochs': 30, 'learning_rate': 0.010260501221891143, 'weight_decay': 3.2980953091156784e-05, 'dropout_rate': 0.1, 'batch_size': 16, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 85.74000000000001.\n",
            "[I 2026-01-30 14:23:29,614] Trial 3 finished with value: 72.05 and parameters: {'num_hidden_layers': 3, 'neurons_per_layer': 8, 'epochs': 40, 'learning_rate': 0.008698759627172582, 'weight_decay': 2.1845478507180475e-05, 'dropout_rate': 0.2, 'batch_size': 16, 'optimizer': 'Adam'}. Best is trial 1 with value: 85.74000000000001.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vJ-J9R5gfW_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolution Neural Network"
      ],
      "metadata": {
        "id": "3fe_yHzTxX7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, features, labels):\n",
        "    self.features = torch.tensor(features, dtype = torch.float32).reshape(-1,1,28,28)\n",
        "    self.labels = torch.tensor(labels, dtype = torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.features[index], self.labels[index]"
      ],
      "metadata": {
        "id": "_5cyhf4pH9jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "test_dataset = CustomDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "0lA8XTAzyBgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import MaxPool2d\n",
        "class MyNN(nn.Module):\n",
        "\n",
        "  def __init__(self, input_features):\n",
        "    super().__init__()\n",
        "    self.features = nn.Sequential(\n",
        "        nn.Conv2d(1, 32, kernel_size=3, padding='same'),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "        nn.Conv2d(32, 64, kernel_size=3, padding='same'),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(7*7*64, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.4),\n",
        "\n",
        "        nn.Linear(128, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.4),\n",
        "\n",
        "        nn.Linear(64, 10)\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = self.features(X)\n",
        "    X = self.classifier(X)\n",
        "    return X"
      ],
      "metadata": {
        "id": "XmIohY2wyK39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "\n",
        "model = MyNN(1)\n",
        "model.to(device)\n",
        "\n",
        "criterion  = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = learning_rate, weight_decay=0.001)"
      ],
      "metadata": {
        "id": "Lpb0W5k55MVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  total_epoch_loss=0\n",
        "\n",
        "  for batch_features, batch_labels in train_loader:\n",
        "\n",
        "    batch_features = batch_features.to(device)\n",
        "    batch_labels = batch_labels.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    outputs = model(batch_features)\n",
        "\n",
        "    # loss\n",
        "    loss = criterion(outputs, batch_labels)\n",
        "\n",
        "    # back pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # update grads\n",
        "    optimizer.step()\n",
        "\n",
        "    total_epoch_loss = total_epoch_loss + loss.item()\n",
        "\n",
        "  avg_loss = total_epoch_loss/len(train_loader)\n",
        "  print(f'Epoch: {epoch+1}, Loss: {avg_loss}')\n",
        "\n"
      ],
      "metadata": {
        "id": "ZHM4JV-15PO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be14f53-7fb6-4639-c01a-364d13085e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 1.5295624668121337\n",
            "Epoch: 2, Loss: 0.8667861475308736\n",
            "Epoch: 3, Loss: 0.7010966714064281\n",
            "Epoch: 4, Loss: 0.6259633712848027\n",
            "Epoch: 5, Loss: 0.5769741468588511\n",
            "Epoch: 6, Loss: 0.5386706933895747\n",
            "Epoch: 7, Loss: 0.5092511166731517\n",
            "Epoch: 8, Loss: 0.48370834958950676\n",
            "Epoch: 9, Loss: 0.46539886145591736\n",
            "Epoch: 10, Loss: 0.4453888331055641\n",
            "Epoch: 11, Loss: 0.4265836374004682\n",
            "Epoch: 12, Loss: 0.4105974615573883\n",
            "Epoch: 13, Loss: 0.3992057658195496\n",
            "Epoch: 14, Loss: 0.3851772902488709\n",
            "Epoch: 15, Loss: 0.3768309440414111\n",
            "Epoch: 16, Loss: 0.369766180284818\n",
            "Epoch: 17, Loss: 0.3557282606581847\n",
            "Epoch: 18, Loss: 0.34772603200872737\n",
            "Epoch: 19, Loss: 0.34096493636369707\n",
            "Epoch: 20, Loss: 0.33548321678241094\n",
            "Epoch: 21, Loss: 0.32839829950332644\n",
            "Epoch: 22, Loss: 0.3229687779088815\n",
            "Epoch: 23, Loss: 0.3178904294490814\n",
            "Epoch: 24, Loss: 0.3126113896270593\n",
            "Epoch: 25, Loss: 0.3041664656539758\n",
            "Epoch: 26, Loss: 0.3036380447367827\n",
            "Epoch: 27, Loss: 0.29830633973081905\n",
            "Epoch: 28, Loss: 0.2942724787294865\n",
            "Epoch: 29, Loss: 0.29071956361730894\n",
            "Epoch: 30, Loss: 0.28601021043260894\n",
            "Epoch: 31, Loss: 0.2803964365899563\n",
            "Epoch: 32, Loss: 0.27671923161149026\n",
            "Epoch: 33, Loss: 0.27329229421615603\n",
            "Epoch: 34, Loss: 0.26840132377147674\n",
            "Epoch: 35, Loss: 0.2670900657514731\n",
            "Epoch: 36, Loss: 0.2628094459394614\n",
            "Epoch: 37, Loss: 0.26111457133491833\n",
            "Epoch: 38, Loss: 0.25578674018184344\n",
            "Epoch: 39, Loss: 0.25351793128053346\n",
            "Epoch: 40, Loss: 0.2509650752385457\n",
            "Epoch: 41, Loss: 0.24972181030809878\n",
            "Epoch: 42, Loss: 0.24570032602945963\n",
            "Epoch: 43, Loss: 0.24555674061576524\n",
            "Epoch: 44, Loss: 0.2400516915857792\n",
            "Epoch: 45, Loss: 0.23673863946894805\n",
            "Epoch: 46, Loss: 0.2349142893354098\n",
            "Epoch: 47, Loss: 0.23315198771158854\n",
            "Epoch: 48, Loss: 0.23139548349678515\n",
            "Epoch: 49, Loss: 0.22869402027726174\n",
            "Epoch: 50, Loss: 0.224601907902956\n",
            "Epoch: 51, Loss: 0.22327546114126842\n",
            "Epoch: 52, Loss: 0.2222915438870589\n",
            "Epoch: 53, Loss: 0.21970560721606017\n",
            "Epoch: 54, Loss: 0.21413384885489942\n",
            "Epoch: 55, Loss: 0.2136787942737341\n",
            "Epoch: 56, Loss: 0.2111794746518135\n",
            "Epoch: 57, Loss: 0.209170708411932\n",
            "Epoch: 58, Loss: 0.20648719211220742\n",
            "Epoch: 59, Loss: 0.20457645079890888\n",
            "Epoch: 60, Loss: 0.2037031153788169\n",
            "Epoch: 61, Loss: 0.20127281694014867\n",
            "Epoch: 62, Loss: 0.1981556701918443\n",
            "Epoch: 63, Loss: 0.19617467382947604\n",
            "Epoch: 64, Loss: 0.19280448666016262\n",
            "Epoch: 65, Loss: 0.19026178705692293\n",
            "Epoch: 66, Loss: 0.1881952739497026\n",
            "Epoch: 67, Loss: 0.18732445338517428\n",
            "Epoch: 68, Loss: 0.18724388082772495\n",
            "Epoch: 69, Loss: 0.1838137339125077\n",
            "Epoch: 70, Loss: 0.18163715496460597\n",
            "Epoch: 71, Loss: 0.1797815762490034\n",
            "Epoch: 72, Loss: 0.17881878328224024\n",
            "Epoch: 73, Loss: 0.17709129584232966\n",
            "Epoch: 74, Loss: 0.17454511896073818\n",
            "Epoch: 75, Loss: 0.17140422567129135\n",
            "Epoch: 76, Loss: 0.1706797522465388\n",
            "Epoch: 77, Loss: 0.16957893008341393\n",
            "Epoch: 78, Loss: 0.16949104332427184\n",
            "Epoch: 79, Loss: 0.16464937003652255\n",
            "Epoch: 80, Loss: 0.1630303329239289\n",
            "Epoch: 81, Loss: 0.16396088004906972\n",
            "Epoch: 82, Loss: 0.16194502183596293\n",
            "Epoch: 83, Loss: 0.1594409961198767\n",
            "Epoch: 84, Loss: 0.15882402784973382\n",
            "Epoch: 85, Loss: 0.15684583921184142\n",
            "Epoch: 86, Loss: 0.15307924605756998\n",
            "Epoch: 87, Loss: 0.15192194544225932\n",
            "Epoch: 88, Loss: 0.1508851052954793\n",
            "Epoch: 89, Loss: 0.1501255842526754\n",
            "Epoch: 90, Loss: 0.14861424023211003\n",
            "Epoch: 91, Loss: 0.1473658665046096\n",
            "Epoch: 92, Loss: 0.14468096479922532\n",
            "Epoch: 93, Loss: 0.1424604737997055\n",
            "Epoch: 94, Loss: 0.1418426293010513\n",
            "Epoch: 95, Loss: 0.1402817915827036\n",
            "Epoch: 96, Loss: 0.14018558637946843\n",
            "Epoch: 97, Loss: 0.13753289318780104\n",
            "Epoch: 98, Loss: 0.1364455728456378\n",
            "Epoch: 99, Loss: 0.13622382816026607\n",
            "Epoch: 100, Loss: 0.13424745250095924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set model for evaluation\n",
        "model.eval()\n",
        "\n",
        "# evaluation code\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for batch_features, batch_labels in test_loader:\n",
        "\n",
        "    batch_features = batch_features.to(device)\n",
        "    batch_labels = batch_labels.to(device)\n",
        "\n",
        "    outputs = model(batch_features)\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    total = total + batch_labels.shape[0]\n",
        "\n",
        "    correct = correct + (predicted == batch_labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct/total*100}')\n"
      ],
      "metadata": {
        "id": "EQTESm_46I8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a715a1-26c9-4d25-a732-9f0df64314db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 92.25999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-trained VGG-16 Model\n"
      ],
      "metadata": {
        "id": "hQyHWPfySFjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transfromations\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "custom_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "ViMwE9KdDvDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, features, labels, transform):\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # resize to (28,28)\n",
        "    image = self.features[index].reshape(28,28)\n",
        "    # change datatype to uint8\n",
        "    image = image.astype(np.uint8)\n",
        "    # change B&W to color\n",
        "    image = np.stack([image]*3, axis=-1)\n",
        "    # convert array to PIL image\n",
        "    image = Image.fromarray(image)\n",
        "    # apply transformations\n",
        "    image = self.transform(image)\n",
        "\n",
        "    return image, torch.tensor(self.labels[index], dtype=torch.long)\n",
        "\n"
      ],
      "metadata": {
        "id": "pYiWR1xfTWkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(X_train, y_train, transform = custom_transform)\n",
        "test_dataset = CustomDataset(X_test, y_test, transform = custom_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, pin_memory=True)"
      ],
      "metadata": {
        "id": "Ax1V89ehb5fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch the pretrained model\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "vgg16 = models.vgg16(pretrained=True)"
      ],
      "metadata": {
        "id": "amb00td9cmsI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a664efad-c8ce-4aa4-b49a-e11ef583aa68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 528M/528M [00:07<00:00, 71.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxxXcVeaCsbb",
        "outputId": "743859a6-7e08-418a-d745-72c1f813ae5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Dropout(p=0.5, inplace=False)\n",
              "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (4): ReLU(inplace=True)\n",
              "  (5): Dropout(p=0.5, inplace=False)\n",
              "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in vgg16.features.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "5Z5oJ-CJDGve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16.classifier = nn.Sequential(\n",
        "    nn.Linear(25088, 1024),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(1024, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 10)\n",
        ")"
      ],
      "metadata": {
        "id": "xNTTOsDkDPUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16 = vgg16.to(device)"
      ],
      "metadata": {
        "id": "Ld0Qv-esDjFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0001\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "lVML0AgXD33G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion  = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vgg16.classifier.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "09yh0sV9EvfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  total_epoch_loss=0\n",
        "\n",
        "  for batch_features, batch_labels in train_loader:\n",
        "\n",
        "    batch_features = batch_features.to(device)\n",
        "    batch_labels = batch_labels.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    outputs = vgg16(batch_features)\n",
        "\n",
        "    # loss\n",
        "    loss = criterion(outputs, batch_labels)\n",
        "\n",
        "    # back pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # update grads\n",
        "    optimizer.step()\n",
        "\n",
        "    total_epoch_loss = total_epoch_loss + loss.item()\n",
        "\n",
        "  avg_loss = total_epoch_loss/len(train_loader)\n",
        "  print(f'Epoch: {epoch+1}, Loss: {avg_loss}')\n",
        "\n"
      ],
      "metadata": {
        "id": "_kgqkLZFEe58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set model for evaluation\n",
        "vgg16.eval()\n",
        "\n",
        "# evaluation code\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for batch_features, batch_labels in test_loader:\n",
        "\n",
        "    batch_features = batch_features.to(device)\n",
        "    batch_labels = batch_labels.to(device)\n",
        "\n",
        "    outputs = vgg16(batch_features)\n",
        "\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    total = total + batch_labels.shape[0]\n",
        "\n",
        "    correct = correct + (predicted == batch_labels).sum().item()\n",
        "\n",
        "print(f'Accuracy: {correct/total*100}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XT6iMSLEx7l",
        "outputId": "f31c9133-616b-43aa-b001-f747130e5bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 93.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VcLnPNAINlX2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}